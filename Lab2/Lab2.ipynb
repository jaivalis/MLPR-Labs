{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Code to be tested before moved to the report"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import gzip, cPickle\n",
      "\n",
      "def load_mnist():\n",
      "\tf = gzip.open('mnist.pkl.gz', 'rb')\n",
      "\tdata = cPickle.load(f)\n",
      "\tf.close()\n",
      "\treturn data\n",
      "\n",
      "def plot_digits(data, numcols, shape=(28,28)):\n",
      "    numdigits = data.shape[0]\n",
      "    numrows = int(numdigits/numcols)\n",
      "    for i in range(numdigits):\n",
      "        plt.subplot(numrows, numcols, i)\n",
      "        plt.axis('off')\n",
      "        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')\n",
      "    plt.show()\n",
      "\n",
      "def plot_digit(digit) :\n",
      "    plt.imshow(digit.reshape(28,28), interpolation='nearest', cmap='Greys')\n",
      "    plt.show()\n",
      "\n",
      "(x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()\n",
      "#plot_digits(x_train[0:8], numcols=4)\n",
      "#print x_train[0]\n",
      "plot_digit(x_train[1])\n",
      "print t_train[1]\n",
      "\n",
      "print shape(x_train)\n",
      "print shape(t_train)\n",
      "print shape(x_valid)\n",
      "print shape(t_valid)\n",
      "print shape(x_test)\n",
      "print shape(t_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niD0XAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x626dba8>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "(50000L, 784L)\n",
        "(50000L,)\n",
        "(10000L, 784L)\n",
        "(10000L,)\n",
        "(10000L, 784L)\n",
        "(10000L,)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "/Code to be tested before moved to the report"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lab\u00a02: Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part\u00a01. Multiclass\u00a0logistic\u00a0regression"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.1\u00a0Gradient\u00adbased\u00a0stochastic\u00a0optimization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.1.1\u00a0Derive\u00a0gradient\u00a0equations"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Equations here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For \n",
      "$$ j=t^{(i)} : \\delta_j^q = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\log p_j} \\frac{\\partial \\log p_j}{\\partial \\log q_j} + \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\log Z} \\frac{\\partial \\log Z}{\\partial Z} \\frac{\\partial Z}{\\partial \\log q_j} = 1 \\cdot 1 - \\frac{\\partial \\log Z}{\\partial Z} \\frac{\\partial Z}{\\partial \\log q_j} = 1 - \\frac{\\partial \\log Z}{\\partial Z} \\frac{\\partial Z}{\\partial \\log q_j} = 1 - \\frac{1}{Z} exp(\\log  q_j)$$\n",
      "\n",
      "\n",
      "For $$j \\neq t^{(i)}: \\delta_{j}^q = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\log Z} \\frac{\\partial \\log Z}{\\partial Z} \\frac{\\partial Z}{\\partial \\log q_j} = - \\frac{\\partial \\log Z}{\\partial Z} \\frac{\\partial Z}{\\partial \\log q_j} = - \\frac{1}{Z} exp(\\log  q_j)$$\n",
      "\n",
      "\n",
      "$$\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial b_j} = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\log q_j} \\frac {\\partial \\log q_j}{\\partial b_j} = \\delta_j^q \\cdot 1 = \\delta_j^q$$\n",
      "\n",
      "$$\\nabla_b\\mathcal{L}^{(i)}= \\delta^q = \\frac{1}{Z} exp(\\log  q_j)$$ \n",
      "\n",
      "$$\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial W_{ij}} = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\log q_j} \\frac {\\partial \\log q_j}{\\partial W_{ij}} = \\delta_j^q \\frac {\\partial \\log q_j}{\\partial W_{ij}} = \\frac{1}{Z} exp(w_{ij}x + b)x$$\n",
      "\n",
      "$$\\nabla_{W_{ij}} \\mathcal{L}^{(i)} = \\frac{1}{Z} exp(w_{ij}x + b)x$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.1.2 Implement gradient computations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "# Write a function logreg_gradient(x,t,w,b) that returns the gradient \n",
      "# with respect to the parameters w and b)\n",
      "# of the log-likelihood for a single datapoint (x,t)\n",
      "\n",
      "# x is a vector: 784         (datapoint)\n",
      "# t is a scalar              (corresponding class)\n",
      "# W is a matrix: 784 x 10    (weights)\n",
      "# b is a vector: 10          (bias)\n",
      "#\n",
      "# returns a matrix: 784 x 10 (partial derivative of log likelihood wrt w)\n",
      "# returns a vector: 10       (partial derivative of log likelihood wrt b)\n",
      "def logreg_gradient(x, t, W, b):\n",
      "    log_Q = []\n",
      "    log_P = []\n",
      "    partial_derivative_logLikelihood_b = []\n",
      "    partial_derivative_logLikelihood_W = []\n",
      "    # log_Q -> Z -> log_P -> delta\n",
      "\n",
      "    # log_Q = the unnormalized probability of the class j\n",
      "    # log_Q is supposed to be a 10 x 1 vector\n",
      "    \n",
      "    log_Q = np.dot(np.transpose(W), x) + b\n",
      "    \n",
      "    Z = 0.0\n",
      "    for i in range (0, 10): # Z = normalizing factor        \n",
      "        Z += math.exp(log_Q[i])\n",
      "        \n",
      "    for j in range(0,10):\n",
      "        # log_P = networs output; contains conditional log probabilities for each digit class j\n",
      "        # log_P is supposed to be a 10 x 1 vector\n",
      "        log_P.append(log_Q[j] - math.log1p(Z))\n",
      "        \n",
      "    for j in range(0,10):\n",
      "        # partial_derivative_logLikelihood_b = derivative of log(Z) w.r.t. Z * derivative of Z w.r.t. log_Q\n",
      "        # partial_derivative_logLikelihood_b is supposed to be a 10 x 1 vector\n",
      "        partial_derivative_logLikelihood_b.append(1/Z * math.exp(log_Q[j]))\n",
      "    # partial_derivative_logLikelihood_W = partial_derivative_logLikelihood_b * derivative of log(q) w.r.t. w\n",
      "    # partial_derivative_logLikelihood_W is supposed to be a 784 x 10 matrix\n",
      "    \n",
      "    partial_derivative_logLikelihood_W = np.outer(x, partial_derivative_logLikelihood_b)\n",
      "    \n",
      "    return partial_derivative_logLikelihood_W, partial_derivative_logLikelihood_b\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "    1.1.3 Stochastic gradient descent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# x_train is a matrix: 50000 x 784 (dataset)\n",
      "# t_train is a vector: 50000       (class of x)\n",
      "# W       is a matrix: 784 x 10    (weights)\n",
      "# b       is a vector: 10          (bias for each class)\n",
      "def sgd_iter(x_train, t_train, W, b):\n",
      "    alpha = 1E-4      # learning rate\n",
      "    \n",
      "    x_trainIndex = np.arange(len(x_train), dtype = int)\n",
      "    np.random.shuffle(x_trainIndex) # shuffle indices\n",
      "\n",
      "    for xIndex in x_trainIndex:\n",
      "        x = x_train[xIndex]\n",
      "        t = t_train[xIndex]\n",
      "        \n",
      "        W = W + alpha * logreg_gradient(x, t, W, b)[0]\n",
      "        \n",
      "    return W\n",
      "\n",
      "W = zeros((784,10))\n",
      "b = np.array( [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "(x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()\n",
      "W = sgd_iter(x_train, t_train, W, b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.2.1\u00a0Train"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_training(handful):\n",
      "    (x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()\n",
      "    for i in range(0,handful):\n",
      "        W = sgd_iter(x_train, t_train, W, b)\n",
      "    \n",
      "        \n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.2.2\u00a0Visualize\u00a0weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.2.3.\u00a0Visualize\u00a0the\u00a08\u00a0hardest\u00a0and\u00a08\u00a0easiest\u00a0digits"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part\u00a02.\u00a0Multilayer\u00a0perceptron"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.1\u00a0Derive\u00a0gradient\u00a0equations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2\u00a0MAP\u00a0optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.3.\u00a0Extra (lol as if..)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2.3.1.\u00a010% bonus\u00a0points:\u00a0Implement\u00a0and train\u00a0a MLP"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2.3.2.\u00a010%\u00a0bonus\u00a0points:\u00a0Less\u00a0than\u00a0150\u00a0misclassifications\u00a0on\u00a0the\u00a0test\u00a0set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}