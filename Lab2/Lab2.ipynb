{
 "metadata": {
  "name": "Lab2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lab\u00a02: Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part\u00a01. Multiclass\u00a0logistic\u00a0regression"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.1\u00a0Gradient\u00adbased\u00a0stochastic\u00a0optimization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.1.1\u00a0Derive\u00a0gradient\u00a0equations"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Equations here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For \n",
      "$$ j=t^{(i)} : \\delta_j^q = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\log p_j} \\frac{\\partial \\log p_j}{\\partial \\log q_j} + \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\log Z} \\frac{\\partial \\log Z}{\\partial Z} \\frac{\\partial Z}{\\partial \\log q_j} = 1 \\cdot 1 - \\frac{\\partial logZ}{\\partial Z} \\frac{\\partial Z}{\\partial \\log q_j} = 1 - \\frac{partial \\log Z}{\\partial Z}$$\n",
      "\n",
      "$$ \\frac{\\partial Z}{\\partial \\log q_j}  = exp(\\log q_i) $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import gzip, cPickle\n",
      "\n",
      "def load_mnist():\n",
      "\tf = gzip.open('mnist.pkl.gz', 'rb')\n",
      "\tdata = cPickle.load(f)\n",
      "\tf.close()\n",
      "\treturn data\n",
      "\n",
      "def plot_digits(data, numcols, shape=(28,28)):\n",
      "    numdigits = data.shape[0]\n",
      "    numrows = int(numdigits/numcols)\n",
      "    for i in range(numdigits):\n",
      "        plt.subplot(numrows, numcols, i)\n",
      "        plt.axis('off')\n",
      "        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')\n",
      "    plt.show()\n",
      "\n",
      "(x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()\n",
      "plot_digits(x_train[0:8], numcols=4)\n",
      "print x_train[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.1.2 Implement gradient computations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "# Write a function logreg_gradient(x,t,w,b) that returns the gradient \n",
      "# with respect to the parameters w and b)\n",
      "# of the log-likelihood for a single datapoint (x,t)\n",
      "def logreg_gradient(x,t,w,b):\n",
      "    log_Q = []\n",
      "    Z = 0.0\n",
      "    log_P = []\n",
      "    partial_derivative_logLikelihood_b = []\n",
      "    partial_derivative_logLikelihood_W = []\n",
      "    # log_Q -> Z -> log_P -> delta\n",
      "    # 10 different classes\n",
      "    for j in range(0,10):\n",
      "        # log_Q = the unnormalized probability of the class j\n",
      "        log_Q[j] = np.dot(np.transpose(w[j]),x) + b[j]\n",
      "    for k in range(0,10):\n",
      "        # Z = normalizing factor\n",
      "        Z += math.exp(log_Q[k])\n",
      "    for j in range(0,10):\n",
      "        # log_P = networs output; contains conditional log probabilities for each digit class j\n",
      "        log_P[j] = log_Q[j] - math.log1p(Z)\n",
      "    for j in range(0,10):\n",
      "        # partial_derivative_logLikelihood_b = derivative of log(Z) w.r.t. Z * derivative of Z w.r.t. log_Q\n",
      "        partial_derivative_logLikelihood_b[j] = 1/Z * (math.exp(log_Q[j])\n",
      "        # partial_derivative_logLikelihood_W = partial_derivative_logLikelihood_b * derivative of log(q) w.r.t. w\n",
      "        partial_derivative_logLikelihood_W[j] = partial_derivative_logLikelihood_b * x\n",
      "    return partial_derivative_logLikelihood_W, partial_derivative_logLikelihood_b\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-18-0be87a0eebf4>, line 26)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-0be87a0eebf4>\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    partial_derivative_logLikelihood_W[j] = partial_derivative_logLikelihood_b * x\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "    1.1.3 Stochastic gradient descent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_iter(x_train,t_train,w,b):\n",
      "    # shuffle indices\n",
      "    indices = np.arrange(len(x_train), dtype = int)\n",
      "    np.random.shuffle(indices)\n",
      "    # learning rate\n",
      "    alpha = 0.0001\n",
      "\n",
      "    for i in indices:\n",
      "                   # update weight(maximize)\n",
      "                   w = w + alpha * logreg_gradient(x_train[i],t[train[i],w,b)\n",
      "    return w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-17-796fabaa44ef>, line 10)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-796fabaa44ef>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    w = w + alpha * logreg_gradient(x_train[i],t[train[i],w,b)\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.2.\u00a0Train"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.2.1\u00a0Train"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.2.2\u00a0Visualize\u00a0weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1.2.3.\u00a0Visualize\u00a0the\u00a08\u00a0hardest\u00a0and\u00a08\u00a0easiest\u00a0digits"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part\u00a02.\u00a0Multilayer\u00a0perceptron"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.1\u00a0Derive\u00a0gradient\u00a0equations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2\u00a0MAP\u00a0optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.3.\u00a0Extra (lol as if..)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2.3.1.\u00a010% bonus\u00a0points:\u00a0Implement\u00a0and train\u00a0a MLP"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2.3.2.\u00a010%\u00a0bonus\u00a0points:\u00a0Less\u00a0than\u00a0150\u00a0misclassifications\u00a0on\u00a0the\u00a0test\u00a0set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}