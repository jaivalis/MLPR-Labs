{
 "metadata": {
  "name": "Lab1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 1A: Linear Regression and Overfitting\n",
      "\n",
      "NOTE: Part B will be handed out next week. \n",
      "\n",
      "### Machine Learning and Pattern Recognition, September 2013\n",
      "\n",
      "* The lab exercises should be made in groups of three people, or at most two people.\n",
      "* The deadline is september 22nd (sunday) 23:59.\n",
      "* Part A and B should be handed in together.\n",
      "* Assignment should be sent to N.Hu@uva.nl (Ninhang Hu). The subject line of your email should be \"\\#lab\\_lastname1\\_lastname2\\_lastname3\".\n",
      "* Put your and your teammates' names in the body of the email\n",
      "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"lab01\\_Kingma\\_Hu\", the attached file should be \"lab01\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
      "\n",
      "Notes on implementation:\n",
      "\n",
      "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
      "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
      "* For this lab, your regression solutions should be in closed form, i.e., should not perform iterative gradient-based optimization but find the exact optimum directly."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
      "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
      "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
      "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
      "$\\newcommand{\\by}{\\mathbf{y}}$\n",
      "\n",
      "## Part 1: Polynomial Regression\n",
      "\n",
      "### 1.1. Generate sinusoidal data\n",
      "Write a method `gen_sinusoidal(N)` that generates toy data like in fig 1.2 of the MLPR book. The method should have a parameter $N$, and should return $N$-dimensional vectors $\\bx$ and $\\bt$, where $\\bx$ contains evenly spaced values from 0 to (including) 2$\\pi$, and the elements $t_i$ of $\\bt$ are distributed according to:\n",
      "\n",
      "$$t_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
      "\n",
      "where $x_i$ is the $i$-th elements of $\\bf{x}$, the mean $\\mu = sin(x_i)$ and the standard deviation $\\sigma = 0.2$.\n",
      "\n",
      "### 1.2 Polynomial regression\n",
      "\n",
      "Write a method `fit_polynomial(x, t, M)` that finds the maximum-likelihood solution of an _unregularized_ $M$-th order polynomial for some dataset `x`. The error function to minimize w.r.t. $\\bw$ is:\n",
      "\n",
      "$E(\\bw) = \\frac{1}{2} (\\bPhi\\bw - \\bt)^T(\\bPhi\\bw - \\bt)$\n",
      "\n",
      "where $\\bPhi$ is the _feature matrix_ (or _design matrix_) as explained in the MLPR book, $\\bt$ is the vector of target values. Your method should return a vector $\\bw$ with the maximum-likelihood parameter estimates. \n",
      "\n",
      "### 1.3 Plot\n",
      "Sample a dataset with $N=9$, and fit four polynomials with $M \\in (0, 1, 3, 9)$.\n",
      "For each value of $M$, plot the prediction function, along with the data and the original sine function. The resulting figure should look similar to fig 1.4 of the MLPR book. Note that you can use matplotlib's `plt.pyplot(.)` functionality for creating grids of figures.\n",
      "\n",
      "### 1.4 Regularized linear regression\n",
      "\n",
      "Write a method `fit_polynomial_reg(x, t, M, lamb)` that fits a _regularized_ $M$-th order polynomial to the sinusoidal data, as discussed in the lectures, where `lamb` is the regularization term _lambda_. (Note that 'lambda' cannot be used as a variable name in Python since it has a special meaning). The error function to minimize w.r.t. $\\bw$:\n",
      "\n",
      "$E(\\bw) = \\frac{1}{2} (\\bPhi\\bw - \\bt)^T(\\bPhi\\bw - \\bt) + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}$\n",
      "\n",
      "For background, see section 3.1.4 of the MLPR book.\n",
      "\n",
      "### 1.5 Model selection by cross-validation\n",
      "Use cross-validation to find a good choice of $M$ and $\\lambda$, given a dataset of $N=9$ datapoints generated with `gen_sinusoidal(9)`. You should write a function that tries (loops over) a reasonable range of choices of $M$ and $\\lambda$, and returns the choice with the best cross-validation error. In this case you can use $K=9$ folds, corresponding to _leave-one-out_ crossvalidation.\n",
      "\n",
      "You can let $M \\in (0, 1, ..., 10)$, and let $\\lambda \\in (e^{-10}, e^{-9}, ..., e^{0})$.\n",
      "\n",
      "To get you started, here's a method you can use to generate indices of cross-validation folds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kfold_indices(N, k):\n",
      "    all_indices = np.arange(N,dtype=int)\n",
      "    np.random.shuffle(all_indices)\n",
      "    idx = np.floor(np.linspace(0,N,k+1))\n",
      "    train_folds = []\n",
      "    valid_folds = []\n",
      "    for fold in range(k):\n",
      "        valid_indices = all_indices[idx[fold]:idx[fold+1]]\n",
      "        valid_folds.append(valid_indices)\n",
      "        train_folds.append(np.setdiff1d(all_indices, valid_indices))\n",
      "    return train_folds, valid_folds\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a comprehensible plot of the cross-validation error for each choice of $M$ and $\\lambda$. Highlight the best choice. \n",
      "\n",
      "_Question_: Explain over-fitting and underfitting, illuminated by your plot. Explain the relationship with model bias and model variance.\n",
      "\n",
      "### 1.6 Plot best cross-validated fit\n",
      "\n",
      "For some dataset with $N = 9$, plot the model with the optimal $M$ and $\\lambda$ according to the cross-validation error, using the method you just wrote. Let the plot make clear which $M$ and $\\lambda$ were found.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}